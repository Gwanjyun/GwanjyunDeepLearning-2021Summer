{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c36bf832-f27d-4e6a-ba7a-03f2a2d8534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.functional import F\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision.ops import nms\n",
    "from torchvision.ops import RoIPool\n",
    "\n",
    "from torchvision.ops import boxes as box_ops\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import cv2\n",
    "\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b967ef-7e73-4269-8c0b-55ac7c742791",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device('cuda:4' if use_cuda else 'cpu')\n",
    "x = torch.Tensor([0]).cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d368881-101d-4146-8b9b-c1b9cf3317c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawbbox(img, pre, keep = None, color = '#FF4565'):\n",
    "    img1 = img.copy()\n",
    "    ImageD = ImageDraw.Draw(img1)\n",
    "    for i in range(len(pre['boxes'])):\n",
    "        if keep is not None and i not in keep:\n",
    "            continue\n",
    "        box = pre['boxes'][i]\n",
    "        ImageD.rectangle([(box[0],box[1]),box[2],box[3]],outline= color,width = 2)\n",
    "        cls_dict = { 'A220':0,\n",
    "             'A330':1, \n",
    "             'A320/321':2, \n",
    "             'Boeing737-800':3,\n",
    "             'Boeing787':4,\n",
    "             'ARJ21':5, \n",
    "             'other':6}\n",
    "        \n",
    "        index_dict = {j:i for i,j in cls_dict.items()}\n",
    "        text = ''\n",
    "        try:\n",
    "            label = pre['labels'][i]\n",
    "            text = text + index_dict[int(label)]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            score = pre['scores'][i]\n",
    "            text = text + ':' + str(float(score))[:5]\n",
    "        except:\n",
    "            pass\n",
    "        c = (int(box[0]),int(box[1])-12)\n",
    "        font = ImageFont.truetype('consolab.ttf',size = 11)\n",
    "        b = ImageD.textbbox(c,text,font)\n",
    "\n",
    "        ImageD.rectangle([(b[0],b[1]),b[2]+2,b[3]],fill = color,outline= color)\n",
    "        ImageD.text(c,text,fill = (255,255,255),font = font,stroke_width=0)\n",
    "    return img1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10964da-7526-4058-a5e2-ab559afc3eb8",
   "metadata": {},
   "source": [
    "# 数据集读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8198391a-99d9-40a2-90ce-ff7ae99c617d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'data/SAR_Airplane_Recognition_trainData/trainData/'\n",
    "image_path = path + 'Images/'\n",
    "gt_path = path + 'gt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9d57de-3bf8-40ae-a9f9-7e46d28b89ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsexml(gt_path, filename):\n",
    "    gt_filename = gt_path + filename\n",
    "    gt_sample = ET.parse(gt_filename)\n",
    "\n",
    "    img_filename = gt_sample.find('source').find('filename').text\n",
    "\n",
    "    Objects = gt_sample.find('objects').findall('object')\n",
    "    objects_name = []\n",
    "    bboxes = []\n",
    "    for object in Objects:\n",
    "        object_name = object.find('possibleresult').find('name').text\n",
    "        objects_name.append(object_name)\n",
    "        points = object.find('points').findall('point')\n",
    "        xmin,ymin = points[0].text.split(',')\n",
    "        xmax,ymax = points[2].text.split(',')\n",
    "        bbox = [int(float(xmin)), int(float(ymin)), int(float(xmax)), int(float(ymax))]\n",
    "        bboxes.append(torch.Tensor(bbox))\n",
    "    cls_dict = { 'A220':0,\n",
    "                 'A330':1, \n",
    "                 'A320/321':2, \n",
    "                 'Boeing737-800':3,\n",
    "                 'Boeing787':4,\n",
    "                 'ARJ21':5, \n",
    "                 'other':6}\n",
    "    target = {\n",
    "                'filename':img_filename,\n",
    "                'labels':[cls_dict[i] for i in objects_name],\n",
    "                'boxes':bboxes\n",
    "            }\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9f8b6eb-42f3-4085-b450-81dac01ab54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': '2.tif',\n",
       " 'labels': [6, 0, 6, 5, 6],\n",
       " 'boxes': [tensor([147., 105., 239., 173.]),\n",
       "  tensor([145.,   1., 252.,  73.]),\n",
       "  tensor([157., 228., 239., 293.]),\n",
       "  tensor([242., 879., 307., 936.]),\n",
       "  tensor([312., 882., 383., 934.])]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsexml(gt_path,'2.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94877001-f115-46c6-8b51-fe446ef7b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dir = sorted([i for i in os.listdir(gt_path) if i[-3:] == 'xml'])\n",
    "targets = []\n",
    "for filename in gt_dir:\n",
    "    targets.append(parsexml(gt_path,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958ca43e-f15b-4278-9af3-ead408400840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': '1409.tif',\n",
       " 'labels': [4, 4],\n",
       " 'boxes': [tensor([  1., 398., 119., 532.]), tensor([ 10., 232., 114., 331.])]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[456]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "391990fd-aad7-432b-957e-3be44b2e545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs = []\n",
    "# for target in tqdm(targets, desc = 'Load data'):\n",
    "#     filename = target['filename']\n",
    "#     img = cv2.imread(image_path + filename)\n",
    "#     img = torch.from_numpy(img)\n",
    "#     imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddd03b64-fc73-42e2-b795-f4048445aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAR_dset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path = 'data/SAR_Airplane_Recognition_trainData/trainData/'):\n",
    "        super(SAR_dset, self).__init__()\n",
    "        self.image_path = path + 'Images/'\n",
    "        self.gt_path = path + 'gt/'\n",
    "        self.gt_dir = sorted([i for i in os.listdir(self.gt_path) if i[-3:] == 'xml'])\n",
    "        self.targets = []\n",
    "        for filename in  self.gt_dir:\n",
    "            self.targets.append(self.parsexml(gt_path,filename))\n",
    "        \n",
    "        self.imgs = []\n",
    "        for target in tqdm(self.targets, desc = 'Load data'):\n",
    "            filename = target['filename']\n",
    "            img = cv2.imread(self.image_path + filename)\n",
    "            img = torch.from_numpy(img).permute(2,0,1)/255\n",
    "            self.imgs.append(img)\n",
    "        \n",
    "        self.train_img = self.imgs[:1000]\n",
    "        self.train_targets = self.targets[:1000]\n",
    "        \n",
    "        self.test_img = self.imgs[1000:]\n",
    "        self.test_targets = self.targets[1000:]\n",
    "        self.mode = 'train'\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'train':\n",
    "            return self.train_img[index],self.train_targets[index]\n",
    "        else:\n",
    "            return self.test_img[index],self.test_targets[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return len(self.train_img)\n",
    "        else:\n",
    "            len(self.test_img)\n",
    "    \n",
    "\n",
    "        \n",
    "    def parsexml(self, gt_path, filename):\n",
    "        gt_filename = gt_path + filename\n",
    "        gt_sample = ET.parse(gt_filename)\n",
    "\n",
    "        img_filename = gt_sample.find('source').find('filename').text\n",
    "\n",
    "        Objects = gt_sample.find('objects').findall('object')\n",
    "        objects_name = []\n",
    "        bboxes = []\n",
    "        for object in Objects:\n",
    "            object_name = object.find('possibleresult').find('name').text\n",
    "            objects_name.append(object_name)\n",
    "            points = object.find('points').findall('point')\n",
    "            xmin,ymin = points[0].text.split(',')\n",
    "            xmax,ymax = points[2].text.split(',')\n",
    "            bbox = [int(float(xmin)), int(float(ymin)), int(float(xmax)), int(float(ymax))]\n",
    "            bboxes.append(torch.Tensor(bbox))\n",
    "        cls_dict = { 'A220':0,\n",
    "                     'A330':1, \n",
    "                     'A320/321':2, \n",
    "                     'Boeing737-800':3,\n",
    "                     'Boeing787':4,\n",
    "                     'ARJ21':5, \n",
    "                     'other':6}\n",
    "        target = {\n",
    "                    'filename':img_filename,\n",
    "                    'labels':[cls_dict[i] for i in objects_name],\n",
    "                    'boxes':bboxes\n",
    "                }\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c69c4623-fa56-4c5e-9240-8edc92417fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c3c89ff88f48e8b7f952278db30b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Load data:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sar_dset = SAR_dset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95901e03-2314-4421-840b-8cf295af242b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': '19.tif',\n",
       " 'labels': [4, 4, 4],\n",
       " 'boxes': [tensor([208., 544., 292., 599.]),\n",
       "  tensor([326., 538., 391., 599.]),\n",
       "  tensor([ 89., 557., 161., 599.])]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sar_dset.test_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5e32ad1-da20-4b3e-a93f-d1a29481fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect(batch):\n",
    "#     print(batch)\n",
    "    img,target = [item for item in zip(*batch)]\n",
    "    for i in range(len(target)):\n",
    "        if isinstance(target[i]['boxes'],list):\n",
    "            target[i]['boxes'] = torch.stack(target[i]['boxes'],dim = 0)\n",
    "            target[i]['labels'] = torch.from_numpy(np.array(target[i]['labels'],dtype = np.int64))\n",
    "    return img,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad24c6fb-e67c-490d-9723-2c85b014e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_dataloader = torch.utils.data.DataLoader(sar_dset, batch_size = 16, shuffle = True,collate_fn = collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6872f290-142e-4771-866f-5eec56fb0333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd9087022364f6393fcb6782bb1d740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x,y in tqdm(sar_dataloader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b2c80-ad93-4157-a392-c0ca60bc678d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd1a9de1-9c12-4973-a9f3-dc18d1e04db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = torchvision.models.resnet50(pretrained = True)\n",
    "'''\n",
    "    特征提取网络，不使用FPN\n",
    "'''\n",
    "features_list = [\n",
    "                    resnet_model.conv1,\n",
    "                    resnet_model.bn1,\n",
    "                    resnet_model.relu,\n",
    "                    resnet_model.maxpool,\n",
    "                    resnet_model.layer1,\n",
    "                    resnet_model.layer2,\n",
    "                    resnet_model.layer3,\n",
    "                ]\n",
    "backbone = nn.Sequential(*features_list)\n",
    "backbone.out_channels = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67e4a72f-b09e-45f4-a2ec-208698e95658",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'filename': '1550.tif',\n",
       "  'labels': tensor([6, 0], device='cuda:4'),\n",
       "  'boxes': tensor([[547., 505., 599., 573.],\n",
       "          [545., 386., 599., 473.]], device='cuda:4')},\n",
       " {'filename': '1422.tif',\n",
       "  'labels': tensor([6, 6], device='cuda:4'),\n",
       "  'boxes': tensor([[ 32.,  23.,  60.,  54.],\n",
       "          [ 82., 129., 113., 164.]], device='cuda:4')},\n",
       " {'filename': '1118.tif',\n",
       "  'labels': tensor([6, 5, 6], device='cuda:4'),\n",
       "  'boxes': tensor([[1243.,  641., 1375.,  766.],\n",
       "          [1212.,  811., 1373.,  920.],\n",
       "          [1234.,  981., 1320., 1050.]], device='cuda:4')},\n",
       " {'filename': '1804.tif',\n",
       "  'labels': tensor([5, 5], device='cuda:4'),\n",
       "  'boxes': tensor([[ 17.,  78.,  65., 132.],\n",
       "          [  1., 293.,  61., 356.]], device='cuda:4')},\n",
       " {'filename': '1460.tif',\n",
       "  'labels': tensor([6, 6, 6, 6, 6, 6], device='cuda:4'),\n",
       "  'boxes': tensor([[383., 314., 440., 355.],\n",
       "          [326., 309., 365., 344.],\n",
       "          [235., 476., 282., 532.],\n",
       "          [171., 470., 215., 522.],\n",
       "          [105., 461., 147., 513.],\n",
       "          [ 39., 459.,  80., 511.]], device='cuda:4')},\n",
       " {'filename': '1026.tif',\n",
       "  'labels': tensor([0, 1], device='cuda:4'),\n",
       "  'boxes': tensor([[462., 198., 550., 277.],\n",
       "          [445., 313., 558., 442.]], device='cuda:4')},\n",
       " {'filename': '1609.tif',\n",
       "  'labels': tensor([5, 6, 6], device='cuda:4'),\n",
       "  'boxes': tensor([[373., 233., 556., 326.],\n",
       "          [403., 387., 546., 458.],\n",
       "          [390.,  89., 535., 180.]], device='cuda:4')},\n",
       " {'filename': '1212.tif',\n",
       "  'labels': tensor([3, 3, 5], device='cuda:4'),\n",
       "  'boxes': tensor([[ 961.,  564., 1023.,  658.],\n",
       "          [ 550.,  903.,  602.,  987.],\n",
       "          [ 457.,  907.,  521.,  993.]], device='cuda:4')})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=14,\n",
    "                                                sampling_ratio=2)\n",
    "anchor_generator = torchvision.models.detection.rpn.AnchorGenerator( sizes=((16, 32, 64, 128, 256, 512),),\n",
    "                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "faster_rcnn = torchvision.models.detection.FasterRCNN(backbone,\n",
    "                                        num_classes = 7,\n",
    "                                        min_size = 200,\n",
    "                                        max_size = 5000,\n",
    "                                        rpn_anchor_generator=anchor_generator,\n",
    "                                        box_roi_pool=roi_pooler)\n",
    "faster_rcnn.to(device)\n",
    "x = [i.cuda(device) for i in x]\n",
    "for i in range(len(y)):\n",
    "    y[i]['labels'] = y[i]['labels'].cuda(device)\n",
    "    y[i]['boxes'] = y[i]['boxes'].cuda(device)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a051442-403c-4ad1-8d9e-72c0f1c0ffc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(1.9350, device='cuda:4', grad_fn=<NllLossBackward>),\n",
       " 'loss_box_reg': tensor(0.0126, device='cuda:4', grad_fn=<DivBackward0>),\n",
       " 'loss_objectness': tensor(0.6942, device='cuda:4', grad_fn=<BinaryCrossEntropyWithLogitsBackward>),\n",
       " 'loss_rpn_box_reg': tensor(0.0079, device='cuda:4', grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faster_rcnn(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d43783bc-9219-40b4-9e1f-99f850c1f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_rcnn = torch.load('model/competition1.pt')\n",
    "optimizer = optim.AdamW(faster_rcnn.parameters(), 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60535ee9-2e17-4472-84ba-ce9dee8bd680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f2057c09824988b7ea042a83764a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca49159824ba415d9cf358c8adf21f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935e14575bb445f182aa86f6829a5a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbacbd2ff86749aeb6e23fe7b56b0eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a7b8953b264b7cb86a179f4852719e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1baa3509a794627bfefcb1e964d4587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4f35252147465890f5eafe59fb2441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e78449707a243789167d65e898449b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "faster_rcnn.train()\n",
    "Epoch = 10\n",
    "min_loss = 123000000\n",
    "for epoch in trange(Epoch,desc = 'Epoch'):\n",
    "    myiter = tqdm(sar_dataloader,colour = '#0066FF')\n",
    "    myiter.set_description_str('Training')\n",
    "    loss1 = 0\n",
    "    for x,y in myiter:\n",
    "        x = [i.cuda(device) for i in x]\n",
    "        for i in range(len(y)):\n",
    "            y[i]['labels'] = y[i]['labels'].cuda(device)\n",
    "            y[i]['boxes'] = y[i]['boxes'].cuda(device)\n",
    "        \n",
    "        loss_dict = faster_rcnn(x,y)\n",
    "        loss = sum(loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        myiter.set_postfix(loss = float(loss),epoch = epoch, loss_all = loss1)\n",
    "        loss1 += float(loss)\n",
    "    if loss1 < min_loss:\n",
    "        min_loss = loss1\n",
    "        torch.save(faster_rcnn,'model/competition1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5784e5-c923-4bb3-a560-7b509abff5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_rcnn.eval()\n",
    "index = 480\n",
    "x = [sar_dset.train_img[index].cuda(device)]\n",
    "labels = sar_dset.train_targets[index]\n",
    "targets = faster_rcnn(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be0aab-e269-43e3-8d7c-3f8814a237ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = x[0].permute(1,2,0).cpu().numpy()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b1cd6-3d47-4ed2-9ba6-422c462784fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = Image.fromarray(np.array(img*255,dtype = np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1ac51-23cc-40d3-8541-8b5dedaf8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nonms = drawbbox(img1,targets)\n",
    "drawbbox(img_nonms,labels,color = '#24AA12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029f1c4-8066-4cbf-96a6-d05035774366",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = nms(targets['boxes'], targets['scores'],0.2)\n",
    "img_nms = drawbbox(img1,targets,keep)\n",
    "drawbbox(img_nms,labels,color = '#24AA12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e04886-183a-42f2-9698-fb309ac37f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13dcca-fa4f-4464-b1d0-9f9365a03d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47583d2-3c70-4878-91da-dadf3b281acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466f2556-c97c-496f-b623-00649cbede52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f326a-2ebe-4ba8-9a91-fab8c6b15a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
